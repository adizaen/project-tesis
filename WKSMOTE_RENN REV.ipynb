{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2eNH8HBdXVT"
      },
      "source": [
        "# LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltoqdoNNPiqR"
      },
      "outputs": [],
      "source": [
        "# import dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# library teknik sampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "# library algoritma klasifikasi\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from keras.layers import BatchNormalization, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# library evaluation metric\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from statistics import mean\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JTuiw3HI2R8"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMDd3ANLPuMk"
      },
      "outputs": [],
      "source": [
        "class GenerateDataset:\n",
        "    def get_data(self):\n",
        "        # dataset\n",
        "        URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/spam.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/ionosphere.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/voice.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/parkinson.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/malware.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/mri.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/heart-attack.csv\"\n",
        "        # URL = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/ozone-level.csv\"\n",
        "\n",
        "        # akses dataset melalui URL\n",
        "        dataset = pd.read_csv(URL)\n",
        "\n",
        "        # menentukan data dan kelas\n",
        "        X = dataset.iloc[:, :-1]\n",
        "        y = dataset.iloc[:, -1]\n",
        "\n",
        "        # membagi dataset menjadi data training dan data testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "        # mengembalikan dataset yang sudah displit\n",
        "        return X_train, X_test, y_train, y_test, URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJDMWuZ7I7bG"
      },
      "source": [
        "# PRE-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUwCW5jSI75U"
      },
      "outputs": [],
      "source": [
        "# class yang berisi sekumpulan fungsi untuk melakukan pre-processing\n",
        "\n",
        "class Preprocessing:\n",
        "\n",
        "    # fungsi untuk menangani data bernilai konstan\n",
        "    def handling_constant_value(self, X, y):\n",
        "        constant_value = []\n",
        "\n",
        "        # mengecek jumlah data berbeda (unik) suatu kolom. Jika bernilai 1,\n",
        "        # maka kolom tersebut bernilai konstan (tetap)\n",
        "        for col in X.columns:\n",
        "            if (X[col].nunique() == 1):\n",
        "                constant_value.append(col)\n",
        "\n",
        "        # jika ada kolom dengan nilai konstan, maka akan dihapus\n",
        "        if (len(constant_value) > 0):\n",
        "            for col in constant_value:\n",
        "                # menghapus kolom yang bernilai konstan\n",
        "                X.drop(col, inplace = True, axis = 1)\n",
        "\n",
        "        # mengembalikan dataset\n",
        "        return X, y\n",
        "\n",
        "    # fungsi untuk menangani kolom kosong\n",
        "    def handling_missing_value(self, X, y):\n",
        "        # cek apakah ada nilai kosong atau tidak\n",
        "        is_missing = X.isnull().values.any()\n",
        "\n",
        "        # jika ada nilai kosong (True), maka dilakukan proses fillna menggunakan median\n",
        "        if (is_missing == True):\n",
        "\n",
        "            # membuat list kolom yang memiliki nilai kosong\n",
        "            miss_column = X.columns[X.isnull().any()].tolist()\n",
        "\n",
        "            # perulangan untuk proses fillna\n",
        "            for col in miss_column:\n",
        "                X.fillna({\n",
        "                    col: X[col].median()\n",
        "                }, inplace = True)\n",
        "\n",
        "        # mengembalikan dataset\n",
        "        return X, y\n",
        "\n",
        "    # fungsi untuk label encoding\n",
        "    def label_encoding(self, X, y):\n",
        "        # menghitung jumlah class dalam numpy array\n",
        "        class_counts = Counter(y)\n",
        "\n",
        "        # mencari label class yang memiliki jumlah paling sedikit (minority) dan paling banyak (majority)\n",
        "        majority_class = max(class_counts, key=class_counts.get)\n",
        "        minority_class = min(class_counts, key=class_counts.get)\n",
        "\n",
        "        # mengganti label class\n",
        "        y = y.replace(to_replace = [majority_class, minority_class], value = [0, 1])\n",
        "\n",
        "        # mengembalikan dataset\n",
        "        return X, y\n",
        "\n",
        "    # fungsi untuk membersihkan dataset sebelum masuk tahap normalisasi\n",
        "    def get_clean_dataset(self, X, y):\n",
        "        # memanggil method untuk menangani data konstan dan data kosong\n",
        "        X, y = self.handling_constant_value(X, y)\n",
        "        X, y = self.handling_missing_value(X, y)\n",
        "        X, y = self.label_encoding(X, y)\n",
        "\n",
        "        # mengembalikan dataset\n",
        "        return X, y\n",
        "\n",
        "    # normalisasi dataset ke dalam range tertentu\n",
        "    def normalisasi(self, X, y):\n",
        "        # membuat objek dari class MinMaxScaler\n",
        "        scaler = MinMaxScaler(feature_range = (0.1, 0.9))\n",
        "\n",
        "        # get kolom dataset\n",
        "        attribute_name = X.columns\n",
        "\n",
        "        # proses fit dan transform pada dataset\n",
        "        X[attribute_name] = scaler.fit_transform(X[attribute_name])\n",
        "\n",
        "        # mengembalikan dataset\n",
        "        return X, y, attribute_name, scaler\n",
        "\n",
        "    # proses seleksi fitur untuk membuat fitur yang tidak relevan\n",
        "    def seleksi_fitur(self, X, y):\n",
        "        # menggabungkan X_train dan y_train\n",
        "        class_target = pd.Series(y, name=\"class\")\n",
        "        X = pd.concat([X, class_target], axis=1)\n",
        "\n",
        "        # inisiasi kelas target\n",
        "        target = X.columns[-1]\n",
        "\n",
        "        # menghitung korelasi r antara semua fitur dengan kelas target\n",
        "        correlation_value = abs(X.corr()[target])\n",
        "\n",
        "        # mengurutkan nilai r dari yang paling besar (paling berpengaruh terhadap kelas target)\n",
        "        most_impact_attribute = correlation_value.sort_values(ascending=False)\n",
        "\n",
        "        # menghapus atribut dengan nilai r yang paling besar (nilai r = 1 yaitu dirinya sendiri)\n",
        "        most_impact_attribute = most_impact_attribute.drop(most_impact_attribute.index[0])\n",
        "\n",
        "        # menyimpan nama atribut yang sudah diurutkan ke dalam list\n",
        "        all_columns = most_impact_attribute.index.tolist()\n",
        "\n",
        "        # duplikasi nama atribut ke variabel baru\n",
        "        available_columns = all_columns.copy()\n",
        "\n",
        "        # inisialisasi list untuk menampung nama kolom yang akan dihapus\n",
        "        columns_to_drop = []\n",
        "\n",
        "        # perulangan untuk seleksi fitur\n",
        "        for i in all_columns:\n",
        "            for j in available_columns:\n",
        "                # mencari nilai korelasi (r) antar variabel\n",
        "                correlation_coefficient = abs(X[i].corr(X[j]))\n",
        "\n",
        "                # logika if jika kolom dirinya sendiri, maka diabaikan\n",
        "                if (i == j):\n",
        "                    continue\n",
        "                elif (correlation_coefficient >= 0.7):\n",
        "                    if not (j in columns_to_drop):\n",
        "                        columns_to_drop.append(j)\n",
        "\n",
        "            # update available_columns tiap 1x iterasi pada all_columns\n",
        "            available_columns.remove(i)\n",
        "\n",
        "        # menghapus kolom pada dataset\n",
        "        X.drop(columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "        # memisah kembali antara X_train dan y_train\n",
        "        X = X.iloc[:, :-1]\n",
        "\n",
        "        # kolom yang digunakan\n",
        "        list_kolom = X.columns\n",
        "\n",
        "        # kembalikan dataset hasil seleksi fitur\n",
        "        return X, y, list_kolom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ETXkGZ-wrid"
      },
      "source": [
        "# SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TJUBwFAIGrC"
      },
      "outputs": [],
      "source": [
        "class FILTERING:\n",
        "    def __init__(self, neighbors):\n",
        "        self.neighbors = neighbors\n",
        "\n",
        "    def fit_transform(self, X, y):\n",
        "        # get bobot untuk tiap fitur\n",
        "        weight, reversed_weight = self.get_attribute_weight(X, y)\n",
        "\n",
        "        # jika ada nilai NaN, ganti dengan 0\n",
        "        weight[np.isnan(weight)] = 0\n",
        "        reversed_weight[np.isnan(reversed_weight)] = 0\n",
        "\n",
        "        # menyimpan nama kolom dari X dan y\n",
        "        X_columns = X.columns.tolist()\n",
        "        y_columns = y.name\n",
        "\n",
        "        # konversi dataframe X dan y menjadi numpy array\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy()\n",
        "\n",
        "        # proses oversampling menggunakan metode WKSMOTE\n",
        "        X, y = self.filter(X, y)\n",
        "\n",
        "        # konversi X numpy array ke dataFrame dan y ke pandas Series\n",
        "        X = pd.DataFrame(X, columns = X_columns)\n",
        "        y = pd.Series(y, name = y_columns)\n",
        "\n",
        "        # kembalikan data X dan y hasil hibrid WKSMOTE-RENN\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def get_attribute_weight(self, X, y):\n",
        "        # menghitung nilai absolut korelasi fitur\n",
        "        weight = abs(X.corrwith(y).to_numpy())\n",
        "\n",
        "        # normalisasi fitur sehingga totalnya 1\n",
        "        normalized_weight = weight /sum(weight)\n",
        "\n",
        "        # menghitung nilai korelasi fitur yang dibalik (reverse)\n",
        "        reversed_weight = sum(weight) / weight\n",
        "\n",
        "        # normalisasi fitur sehingga totalnya 1\n",
        "        normalized_reversed_weight = reversed_weight / sum(reversed_weight)\n",
        "\n",
        "        # mengembalikan bobot\n",
        "        return normalized_weight, normalized_reversed_weight\n",
        "\n",
        "    def get_minority_majority_data(self, X_train, y_train):\n",
        "        # menghitung jumlah class dalam numpy array\n",
        "        class_counts = Counter(y_train)\n",
        "\n",
        "        # mencari label class yang memiliki jumlah paling sedikit (minority) dan paling banyak (majority)\n",
        "        majority_class = max(class_counts, key=class_counts.get)\n",
        "        minority_class = min(class_counts, key=class_counts.get)\n",
        "\n",
        "        # mencari index dari data yang memiliki label class sama dengan target class\n",
        "        index_majority = np.where(y_train == majority_class)[0]\n",
        "        index_minority = np.where(y_train == minority_class)[0]\n",
        "\n",
        "        # ekstrak data X yang memiliki kelas minoritas dan mayoritas\n",
        "        X_majority = X_train[index_majority]\n",
        "        X_minority = X_train[index_minority]\n",
        "\n",
        "        # ekstrak data y yang memiliki kelas minoritas dan mayoritas\n",
        "        y_majority = y_train[index_majority]\n",
        "        y_minority = y_train[index_minority]\n",
        "\n",
        "        return X_majority, X_minority, y_majority, y_minority, majority_class, minority_class\n",
        "\n",
        "    def filter(self, X, y):\n",
        "        # get minority dan majority data\n",
        "        X_majority, X_minority, y_majority, y_minority, majority_class, minority_class = self.get_minority_majority_data(X, y)\n",
        "\n",
        "        # membuat objek dari class KNN\n",
        "        classifier = KNeighborsClassifier()\n",
        "\n",
        "        # proses fitting\n",
        "        classifier.fit(X, y)\n",
        "\n",
        "        # prediksi data mayoritas\n",
        "        y_predict = classifier.predict(X)\n",
        "\n",
        "        # membandingkan y_true dan y_predict\n",
        "        result = (y == y_predict)\n",
        "        index_remove = [index[0] for index, value in np.ndenumerate(result) if value==False]\n",
        "\n",
        "        # hapus data noise\n",
        "        X_filter = np.delete(X, index_remove, axis=0)\n",
        "        y_filter = np.delete(y, index_remove, axis=0)\n",
        "\n",
        "        # kembalikan data\n",
        "        return X_filter, y_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkahiA28dRQL"
      },
      "outputs": [],
      "source": [
        "class WKSMOTE_RENN:\n",
        "    def __init__(self, neighbors, n_neighbors_filter):\n",
        "        self.neighbors = neighbors\n",
        "        self. n_neighbors_filter =  n_neighbors_filter\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        # get bobot untuk tiap fitur\n",
        "        weight, reversed_weight = self.get_attribute_weight(X, y)\n",
        "\n",
        "        # jika ada nilai NaN, ganti dengan 0\n",
        "        weight[np.isnan(weight)] = 0\n",
        "        reversed_weight[np.isnan(reversed_weight)] = 0\n",
        "\n",
        "        # menyimpan nama kolom dari X dan y\n",
        "        X_columns = X.columns.tolist()\n",
        "        y_columns = y.name\n",
        "\n",
        "        # konversi dataframe X dan y menjadi numpy array\n",
        "        X = X.to_numpy()\n",
        "        y = y.to_numpy()\n",
        "\n",
        "        # proses filter\n",
        "        X, y = self.filter(X, y)\n",
        "\n",
        "        # proses oversampling menggunakan metode WKSMOTE\n",
        "        X, y = self.wksmote(X, y, weight)\n",
        "\n",
        "        # proses undersampling menggunakan metode RENN\n",
        "        X, y = self.renn(X, y, reversed_weight)\n",
        "\n",
        "        # konversi X numpy array ke dataFrame dan y ke pandas Series\n",
        "        X = pd.DataFrame(X, columns = X_columns)\n",
        "        y = pd.Series(y, name = y_columns)\n",
        "\n",
        "        # kembalikan data X dan y hasil hibrid WKSMOTE-RENN\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    def wksmote(self, X, y, weight):\n",
        "        # get data minoritas dan mayoritas\n",
        "        X_majority, X_minority, y_majority, y_minority, majority_class, minority_class  = self.get_minority_majority_data(X, y)\n",
        "\n",
        "        # fit model kNN untuk digunakan dalam mencari k tetangga terdekat\n",
        "        model_neighbors_smote = KNeighborsClassifier(n_neighbors=self.neighbors + 1, metric='minkowski', p=1, metric_params={'w': weight})\n",
        "        model_neighbors_smote = model_neighbors_smote.fit(X_minority, y_minority)\n",
        "\n",
        "        # membuat numpy array untuk menampung data sintetis\n",
        "        X_synthetic_data = np.zeros((0, X_majority.shape[1]))\n",
        "\n",
        "        # index saat ini\n",
        "        current_index = 0\n",
        "\n",
        "        # perulangan untuk membuat data sintetis sampai count_minority == count_majority (seimbang)\n",
        "        while True:\n",
        "\n",
        "            # cari k tetangga terdekat dari data point yang dipilih\n",
        "            data_point = np.array([X_minority[current_index]])\n",
        "            sorted_nearest_distance = model_neighbors_smote.kneighbors(data_point, return_distance=True)\n",
        "\n",
        "            # memisah data jarak dan index dari k tetangga terdekat yang terpilih\n",
        "            distance = list(sorted_nearest_distance[0][0])\n",
        "            index = list(sorted_nearest_distance[1][0])\n",
        "            distance.pop(0)\n",
        "            index.pop(0)\n",
        "\n",
        "            # menggabungkan data jarak dan index menjadi pandas series\n",
        "            sorted_nearest_distance = pd.Series(distance, index = index)\n",
        "\n",
        "            # membuat data sintetis\n",
        "            X_synthetic_data = self.create_synthetic_data(X_synthetic_data, data_point, X_minority, sorted_nearest_distance)\n",
        "\n",
        "            # cek apakah sudah seimbang\n",
        "            balance_status = self.is_balance(y_majority, y_minority, X_synthetic_data)\n",
        "            if balance_status: break\n",
        "\n",
        "            # jika belum seimbang dan index data minority sudah sampai akhir, maka ulangi loop dari awal\n",
        "            if current_index == len(y_minority) - 1:\n",
        "                current_index = 0\n",
        "            else:\n",
        "                current_index += 1\n",
        "\n",
        "        # membuat y_synthetic_data\n",
        "        y_synthetic_data = np.full(X_synthetic_data.shape[0], minority_class)\n",
        "\n",
        "        # menggabungkan X dan y baik untuk minority maupun majority\n",
        "        X_wksmote = np.row_stack((X, X_synthetic_data))\n",
        "        y_wksmote = np.append(y, y_synthetic_data)\n",
        "\n",
        "        # mengembalikan X dan y hasil WKSMOTE\n",
        "        return X_wksmote, y_wksmote\n",
        "\n",
        "    def renn(self, X, y, weight):\n",
        "        # get minority dan majority data\n",
        "        X_majority, X_minority, y_majority, y_minority, majority_class, minority_class = self.get_minority_majority_data(X, y)\n",
        "\n",
        "        # fit model kNN untuk digunakan dalam mencari k tetangga terdekat\n",
        "        model_neighbors_renn = KNeighborsClassifier(n_neighbors=self.neighbors + 1, metric='minkowski', p=1, metric_params={'w': weight})\n",
        "        model_neighbors_renn = model_neighbors_renn.fit(X, y)\n",
        "\n",
        "        # membuat list untuk menampung index data yang akan dihapus\n",
        "        index_remove = []\n",
        "\n",
        "        # index saat ini\n",
        "        current_index = 0\n",
        "\n",
        "        while True:\n",
        "            # cari k tetangga terdekat dari data point yang dipilih\n",
        "            data_point = np.array([X[current_index]])\n",
        "            sorted_nearest_distance = model_neighbors_renn.kneighbors(data_point, return_distance=True)\n",
        "\n",
        "            # memisah data jarak dan index dari k tetangga terdekat yang terpilih\n",
        "            distance = list(sorted_nearest_distance[0][0])\n",
        "            index = list(sorted_nearest_distance[1][0])\n",
        "            distance.pop(0)\n",
        "            index.pop(0)\n",
        "\n",
        "            # menggabungkan data jarak dan index menjadi pandas series\n",
        "            sorted_nearest_distance = pd.Series(distance, index = index)\n",
        "\n",
        "            # menampung index yang akan dihapus\n",
        "            index_remove = self.remove_data(index_remove, X, y, data_point, sorted_nearest_distance)\n",
        "\n",
        "            # jika current index sudah sampai data minoritas terakhir, maka berhenti\n",
        "            if current_index == len(y) - 1:\n",
        "                break\n",
        "            else:\n",
        "                current_index += 1\n",
        "\n",
        "        # menghapus data\n",
        "        X_renn = np.delete(X, index_remove, axis=0)\n",
        "        y_renn = np.delete(y, index_remove, axis=0)\n",
        "\n",
        "        # mengembalikan X dan y hasil RENN\n",
        "        return X_renn, y_renn\n",
        "\n",
        "    def is_balance(self, y_majority, y_minority, X_synthetic_data):\n",
        "        count_minority = len(y_minority) + X_synthetic_data.shape[0]\n",
        "        lower_limit = len(y_majority) - self.neighbors * 2\n",
        "        return True if count_minority >= lower_limit else False\n",
        "\n",
        "    def get_attribute_weight(self, X, y):\n",
        "        # menghitung nilai absolut korelasi fitur\n",
        "        weight = abs(X.corrwith(y).to_numpy())\n",
        "\n",
        "        # normalisasi fitur sehingga totalnya 1\n",
        "        normalized_weight = weight /sum(weight)\n",
        "\n",
        "        # menghitung nilai korelasi fitur yang dibalik (reverse)\n",
        "        reversed_weight = sum(weight) / weight\n",
        "\n",
        "        # normalisasi fitur sehingga totalnya 1\n",
        "        normalized_reversed_weight = reversed_weight / sum(reversed_weight)\n",
        "\n",
        "        # mengembalikan bobot\n",
        "        return normalized_weight, normalized_reversed_weight\n",
        "\n",
        "\n",
        "    def create_synthetic_data(self, X_synthetic_data, data_point, X_safe_minority, sorted_nearest_distance):\n",
        "        random.seed(42)\n",
        "        # perulangan untuk membuat data sintetis sebanyak k neighbors\n",
        "        for i in sorted_nearest_distance.index:\n",
        "            # mencari selisih antara 2 data\n",
        "            different1 = data_point - X_safe_minority[i]\n",
        "            different2 = X_safe_minority[i] - data_point\n",
        "\n",
        "            # membuat data sintetis\n",
        "            new_data1 = X_safe_minority[i] + random.uniform(0.2, 0.4) * different1\n",
        "            new_data2 = X_safe_minority[i] + random.uniform(0.2, 0.4) * different2\n",
        "            new_data = np.vstack((new_data1, new_data2))\n",
        "\n",
        "            # menggabungkan dengan data sintetis yang ada\n",
        "            X_synthetic_data = np.vstack((X_synthetic_data, new_data))\n",
        "\n",
        "        # kembalikan data sintetis yang dibuat\n",
        "        return X_synthetic_data\n",
        "\n",
        "    def remove_data(self, index_remove, X_wksmote, y_wksmote, data_point, sorted_nearest_distance):\n",
        "        # mencari label class pada y berdasarkan index dari sorted_nearest_distance\n",
        "        y_sorted_nearest_distance = y_wksmote[sorted_nearest_distance.index]\n",
        "\n",
        "        # menghitung jumlah class dalam numpy array\n",
        "        class_counts = Counter(y_sorted_nearest_distance)\n",
        "\n",
        "        # mencari label class yang memiliki jumlah paling banyak (majority) dan paling sedikit (minority) dari k tetangga terdekat\n",
        "        majority_class_k_neighbors = max(class_counts, key=class_counts.get)\n",
        "\n",
        "        # mencari index dari data point\n",
        "        data_point_index = np.where((X_wksmote == data_point).all(axis=1))[0][0]\n",
        "\n",
        "        # mencari label class dari data point\n",
        "        data_point_class = y_wksmote[data_point_index]\n",
        "\n",
        "        # cari index nya\n",
        "        if majority_class_k_neighbors != data_point_class:\n",
        "            index_remove.append(data_point_index)\n",
        "\n",
        "            for index in sorted_nearest_distance.index:\n",
        "                if y_wksmote[index] != majority_class_k_neighbors:\n",
        "                    index_remove.append(index)\n",
        "\n",
        "        # kembalikan data sintetis yang dibuat\n",
        "        return index_remove\n",
        "\n",
        "    def get_minority_majority_data(self, X_train, y_train):\n",
        "        # menghitung jumlah class dalam numpy array\n",
        "        class_counts = Counter(y_train)\n",
        "\n",
        "        # mencari label class yang memiliki jumlah paling sedikit (minority) dan paling banyak (majority)\n",
        "        majority_class = max(class_counts, key=class_counts.get)\n",
        "        minority_class = min(class_counts, key=class_counts.get)\n",
        "\n",
        "        # mencari index dari data yang memiliki label class sama dengan target class\n",
        "        index_majority = np.where(y_train == majority_class)[0]\n",
        "        index_minority = np.where(y_train == minority_class)[0]\n",
        "\n",
        "        # ekstrak data X yang memiliki kelas minoritas dan mayoritas\n",
        "        X_majority = X_train[index_majority]\n",
        "        X_minority = X_train[index_minority]\n",
        "\n",
        "        # ekstrak data y yang memiliki kelas minoritas dan mayoritas\n",
        "        y_majority = y_train[index_majority]\n",
        "        y_minority = y_train[index_minority]\n",
        "\n",
        "        return X_majority, X_minority, y_majority, y_minority, majority_class, minority_class\n",
        "\n",
        "    def filter(self, X, y):\n",
        "        # get minority dan majority data\n",
        "        X_majority, X_minority, y_majority, y_minority, majority_class, minority_class = self.get_minority_majority_data(X, y)\n",
        "\n",
        "        # membuat objek dari class KNN\n",
        "        classifier = KNeighborsClassifier(n_neighbors=self.n_neighbors_filter)\n",
        "\n",
        "        # proses fitting\n",
        "        classifier.fit(X, y)\n",
        "\n",
        "        # prediksi data mayoritas\n",
        "        y_predict = classifier.predict(X)\n",
        "\n",
        "        # membandingkan y_true dan y_predict\n",
        "        result = (y == y_predict)\n",
        "        index_remove = [index[0] for index, value in np.ndenumerate(result) if value==False]\n",
        "\n",
        "        # hapus data noise\n",
        "        X_filter = np.delete(X, index_remove, axis=0)\n",
        "        y_filter = np.delete(y, index_remove, axis=0)\n",
        "\n",
        "        # kembalikan data\n",
        "        return X_filter, y_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-z4vnM1b2Dg"
      },
      "outputs": [],
      "source": [
        "class SMOTE_ENN:\n",
        "    def __init__(self, neighbors):\n",
        "        self.neighbors = neighbors\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        smote = SMOTE(k_neighbors=self.neighbors, random_state=42)\n",
        "        enn = EditedNearestNeighbours(n_neighbors=self.neighbors)\n",
        "\n",
        "        # sampling dengan SMOTE-ENN\n",
        "        smoteenn = SMOTEENN(smote=smote, enn=enn)\n",
        "        X_smote_enn, y_smote_enn = smoteenn.fit_resample(X, y)\n",
        "\n",
        "        return X_smote_enn, y_smote_enn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8YeTNTbI-na"
      },
      "source": [
        "# CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLnL0CysJTFg"
      },
      "outputs": [],
      "source": [
        "class Classification:\n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    def naive_bayes(self):\n",
        "        # membuat objek dari class GaussianNB\n",
        "        model = GaussianNB()\n",
        "\n",
        "        # proses training\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # mengembalikan model hasil training\n",
        "        return model\n",
        "\n",
        "    def ann(self):\n",
        "        # membuat objek dari class ANN\n",
        "        model = MLPClassifier(random_state=42, tol=0.001, max_iter=500)\n",
        "\n",
        "        # proses training\n",
        "        model.fit(self.X_train, self.y_train)\n",
        "\n",
        "        # mengembalikan model hasil training\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gzeFsmuJTkZ"
      },
      "source": [
        "# TRAINING & TESTING PROCESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35FgVeDDJVwK"
      },
      "outputs": [],
      "source": [
        "class Train_Test_Process:\n",
        "    def train(self, X_train, y_train, object_class_sampling=None, classifier=\"NB\"):\n",
        "        # konversi X dataframe dan y series ke bentuk numpy array\n",
        "        X = X_train.to_numpy()\n",
        "        y = y_train.to_numpy()\n",
        "\n",
        "        # membuat objek dari class StratifiedKFold\n",
        "        objStratified = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "        # inisialiasi awal untuk menampung hasil evaluasi\n",
        "        list_gmean_stratified = []\n",
        "\n",
        "        # menampung model tiap fold\n",
        "        list_model = []\n",
        "        list_X_train = []\n",
        "        list_X_test = []\n",
        "        list_y_train = []\n",
        "        list_y_test = []\n",
        "\n",
        "        # inisialisasi awal model terbaik\n",
        "        best_model_index = 0\n",
        "\n",
        "        # inisialisasi awal index untuk perulangan\n",
        "        index = 0\n",
        "\n",
        "        for train_index, test_index in objStratified.split(X, y):\n",
        "            # membagi data menjadi train dan test\n",
        "            X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "            y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "            # konversi X array ke dataframe dan y array ke series\n",
        "            X_train_fold = pd.DataFrame(X_train_fold, columns=X_train.columns)\n",
        "            X_test_fold = pd.DataFrame(X_test_fold, columns=X_train.columns)\n",
        "            y_train_fold = pd.Series(y_train_fold)\n",
        "            y_test_fold = pd.Series(y_test_fold)\n",
        "\n",
        "            # lakukan sampling\n",
        "            if object_class_sampling == None:\n",
        "                X_sampling, y_sampling = X_train_fold, y_train_fold\n",
        "            else:\n",
        "                X_sampling, y_sampling = object_class_sampling.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "            # membuat objek dari class Classification\n",
        "            objClassification = Classification(X_sampling, y_sampling)\n",
        "\n",
        "            # membuat model ML\n",
        "            if classifier == \"NB\":\n",
        "                model = objClassification.naive_bayes()\n",
        "            else:\n",
        "                model = objClassification.ann()\n",
        "\n",
        "            # proses prediksi\n",
        "            y_predict = model.predict(X_test_fold)\n",
        "            y_predict = pd.Series(y_predict)\n",
        "\n",
        "            # evaluasi\n",
        "            tn, fp, fn, tp = confusion_matrix(y_test_fold, y_predict).ravel()\n",
        "            recall = tp/(tp + fn)\n",
        "            specificity = tn/(tn + fp)\n",
        "            g_mean = (recall * specificity)**0.5\n",
        "\n",
        "            # gabungkan skor\n",
        "            list_gmean_stratified.append(g_mean)\n",
        "\n",
        "            # menyimpan model dan data validasi\n",
        "            list_model.append(model)\n",
        "            list_X_train.append(X_sampling)\n",
        "            list_X_test.append(X_test_fold)\n",
        "            list_y_train.append(y_sampling)\n",
        "            list_y_test.append(y_test_fold)\n",
        "\n",
        "            # menyimpan model terbaik\n",
        "            if list_gmean_stratified[best_model_index] < g_mean:\n",
        "                best_model_index = index\n",
        "\n",
        "            # increment\n",
        "            index += 1\n",
        "\n",
        "        # retrain ulang model terbaik dengan dataset utuh\n",
        "        model = list_model[best_model_index]\n",
        "\n",
        "        X_train_best = list_X_train[best_model_index]\n",
        "        X_test_best = list_X_test[best_model_index]\n",
        "        y_train_best = list_y_train[best_model_index]\n",
        "        y_test_best = list_y_test[best_model_index]\n",
        "\n",
        "        X_train = pd.concat([X_train_best, X_test_best], axis=0)\n",
        "        y_train = pd.concat([y_train_best, y_test_best], axis=0)\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # kembalikan model terbaik yang sudah di-retrain\n",
        "        return model, X_train, y_train\n",
        "\n",
        "    def test(self, model, X_test, y_test):\n",
        "        # proses predict\n",
        "        y_predict = model.predict(X_test)\n",
        "        y_predict = pd.Series(y_predict)\n",
        "\n",
        "        # evaluasi\n",
        "        tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
        "        accuracy = ((tn + tp)/(tn + tp + fp + fn)) * 100\n",
        "        recall = (tp/(tp + fn)) * 100\n",
        "        specificity = (tn/(tn + fp)) * 100\n",
        "        g_mean = ((recall * specificity)**0.5)\n",
        "\n",
        "        # pembulatan 2 digit\n",
        "        accuracy = round(accuracy, 2)\n",
        "        recall = round(recall, 2)\n",
        "        specificity = round(specificity, 2)\n",
        "        g_mean = round(g_mean, 2)\n",
        "\n",
        "        # membuat list untuk menampung performa\n",
        "        list_performa = [accuracy, recall, specificity, g_mean]\n",
        "\n",
        "        # kembalikan hasil\n",
        "        return list_performa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJsah8Q53Knp"
      },
      "source": [
        "# VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEa88uxJ8Luj"
      },
      "outputs": [],
      "source": [
        "def visualize(list_smote_enn, list_wksmote_renn):\n",
        "    labels = [\"Accuracy\", \"Recall\", \"Specificity\", \"G-Mean\"]\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.30\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    border_color=\"#000000\"\n",
        "    rects1 = ax.bar(x - 0.5 * width, list_smote_enn, width, label='SMOTE-ENN', color=\"#694897\", edgecolor=border_color)\n",
        "    rects2 = ax.bar(x + 0.5 * width, list_wksmote_renn, width, label='WKSMOTE-RENN', color=\"#FEE21D\", edgecolor=border_color)\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_title('Perbandingan Performa')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend(loc=\"lower center\", ncol = 2, bbox_to_anchor=(0.5, -0.2, 0, 0))\n",
        "\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 1),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzU6bUlTqHao"
      },
      "outputs": [],
      "source": [
        "def visualize_v2(list_base, list_smote_enn, list_wksmote_renn):\n",
        "    labels = [\"Accuracy\", \"Recall\", \"Specificity\", \"G-Mean\"]\n",
        "\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.30\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    border_color=\"#000000\"\n",
        "    rects1 = ax.bar(x - width, list_base, width, label='Original', color=\"#00B8BC\", edgecolor=border_color)\n",
        "    rects2 = ax.bar(x, list_smote_enn, width, label='SMOTE-ENN', color=\"#694897\", edgecolor=border_color)\n",
        "    rects3 = ax.bar(x + width, list_wksmote_renn, width, label='WKSMOTE-RENN', color=\"#FEE21D\", edgecolor=border_color)\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_title('Perbandingan Performa')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend(loc=\"lower center\", ncol = 3, bbox_to_anchor=(0.5, -0.2, 0, 0))\n",
        "\n",
        "    def autolabel(rects):\n",
        "        for rect in rects:\n",
        "            height = rect.get_height()\n",
        "            ax.annotate('{}'.format(height),\n",
        "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                        xytext=(0, 1),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "\n",
        "    autolabel(rects1)\n",
        "    autolabel(rects2)\n",
        "    autolabel(rects3)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twEMiXNqxtHl"
      },
      "outputs": [],
      "source": [
        "def show_plot(X_base, X_smote_enn, X_wksmote_renn, y_base, y_smote_enn, y_wksmote_renn, column1, column2):\n",
        "    # set color\n",
        "    colors = [\"#440154\", \"#FC9900\"]\n",
        "\n",
        "    # Create a figure and a grid of subplots\n",
        "    f, axes = plt.subplots(1,3,figsize=(10,5), dpi=100)\n",
        "\n",
        "    # Create scatter plots on each subplot\n",
        "    sns.scatterplot(x = column1, y = column2, data = X_base, hue = y_base, ax = axes[0], s=20, palette=colors)\n",
        "    axes[0].set_title('Data Asli')\n",
        "\n",
        "    sns.scatterplot(x = column1, y = column2, data = X_smote_enn, hue = y_smote_enn, ax = axes[1], s=20, palette=colors)\n",
        "    axes[1].set_title('SMOTE-ENN')\n",
        "\n",
        "    sns.scatterplot(x = column1, y = column2, data = X_wksmote_renn, hue = y_wksmote_renn, ax = axes[2], s=20, palette=colors)\n",
        "    axes[2].set_title('WKSMOTE-RENN')\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNyQxhBGleoA"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(model, X_test, y_test):\n",
        "    actual = y_test\n",
        "    y_predict = model.predict(X_test)\n",
        "    predicted = pd.Series(y_predict)\n",
        "\n",
        "    confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "    cm_display.plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I3PlaBs_Z8z"
      },
      "source": [
        "# WORKING AREA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoR1NDR6L_YX"
      },
      "outputs": [],
      "source": [
        "# # get data training dan data testing dengan memanggil method\n",
        "# X_train, X_test, y_train, y_test, URL = GenerateDataset().get_data()\n",
        "\n",
        "# # encoding\n",
        "# X_train, y_train = Preprocessing().label_encoding(X_train, y_train)\n",
        "\n",
        "# # cleaning data\n",
        "# X_train, y_train = Preprocessing().get_clean_dataset(X_train, y_train)\n",
        "\n",
        "# # implementasi normalisasi data pada data latih\n",
        "# X_train, y_train, list_column_norm, scaler = Preprocessing().normalisasi(X_train, y_train)\n",
        "\n",
        "# # implementasi seleksi fitur\n",
        "# X_train, y_train, list_column_selected = Preprocessing().seleksi_fitur(X_train, y_train)\n",
        "\n",
        "# # membuang fitur yang tidak digunakan pada proses preprocessing\n",
        "# X_test = X_test[list_column_norm]\n",
        "\n",
        "# # encoding\n",
        "# X_test, y_test = Preprocessing().label_encoding(X_test, y_test)\n",
        "\n",
        "# # implementasi normalisasi data pada data uji\n",
        "# X_test, y_test = pd.DataFrame(scaler.transform(X_test), columns=list_column_norm), y_test.reset_index(drop=True)\n",
        "\n",
        "# # menggunakan kolom yang akan digunakan saja\n",
        "# X_test, y_test = X_test[list_column_selected], y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmPub4AsaTEd"
      },
      "outputs": [],
      "source": [
        "# object dari class beberapa algoritma sampling\n",
        "# smote_enn = SMOTE_ENN(neighbors=5)\n",
        "# wksmote_renn = WKSMOTE_RENN(neighbors=5)\n",
        "\n",
        "# proses training dengan menerapkan beberapa algoritma sampling di naive bayes\n",
        "# model_base, X_base, y_base = Train_Test_Process().train(X_train, y_train, classifier=\"NB\")\n",
        "# model_smote_enn, X_smote_enn, y_smote_enn = Train_Test_Process().train(X_train, y_train, smote_enn, classifier=\"NB\")\n",
        "# model_wksmote_renn, X_wksmote_renn, y_wksmote_renn = Train_Test_Process().train(X_train, y_train, wksmote_renn, classifier=\"NB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkYH8JRFcPEG"
      },
      "outputs": [],
      "source": [
        "# proses testing\n",
        "# result_base = Train_Test_Process().test(model_base, X_base, y_base)\n",
        "# result_smote_enn = Train_Test_Process().test(model_smote_enn, X_test, y_test)\n",
        "# result_wksmote_renn = Train_Test_Process().test(model_wksmote_renn, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZqfKTU6Y-Da"
      },
      "source": [
        "# HASIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DG2IFRPTSPm"
      },
      "outputs": [],
      "source": [
        "# visualize(result_smote_enn, result_wksmote_renn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd4Xz53dmUH9"
      },
      "outputs": [],
      "source": [
        "# show_confusion_matrix(model_wksmote_renn, X_test, y_test)\n",
        "# print(\"accuracy: \", result_wksmote_renn[0])\n",
        "# print(\"recall: \", result_wksmote_renn[1])\n",
        "# print(\"specificity: \", result_wksmote_renn[2])\n",
        "# print(\"gmean: \", result_wksmote_renn[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guWn_pzspVNs"
      },
      "outputs": [],
      "source": [
        "# URL_DATA = URL.split(\"/\")[-1]\n",
        "\n",
        "# if URL_DATA == 'spam.csv':\n",
        "#     column1 = \"word_freq_all\"\n",
        "#     column2 = \"char_freq_$\"\n",
        "# elif URL_DATA == 'ionosphere.csv':\n",
        "#     column1 = \"V8\"\n",
        "#     column2 = \"V30\"\n",
        "# elif URL_DATA == 'colon-cancer.csv':\n",
        "#     column1 = \"V53\"\n",
        "#     column2 = \"V30\"\n",
        "# elif URL_DATA == 'voice.csv':\n",
        "#     column1 = \"Jitter->pitch_PQ5_classical_Baken\"\n",
        "#     column2 = \"HNR->HNR_dB_Praat_std\"\n",
        "# elif URL_DATA == 'parkinson.csv':\n",
        "#     column1 = \"f1\"\n",
        "#     column2 = \"tqwt_kurtosisValue_dec_21\"\n",
        "# elif URL_DATA == 'mri.csv':\n",
        "#     column1 = \"obstime\"\n",
        "#     column2 = \"plt\"\n",
        "# elif URL_DATA == 'heart-attack.csv':\n",
        "#     column1 = \"BMI\"\n",
        "#     column2 = \"PhysHlth\"\n",
        "# elif URL_DATA == 'ozone-level.csv':\n",
        "#     column1 = \"V12\"\n",
        "#     column2 = \"V59\"\n",
        "\n",
        "# show_plot(X_train, X_smote_enn, X_wksmote_renn, y_train, y_smote_enn, y_wksmote_renn, column1, column2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_bq01X-5iyG"
      },
      "source": [
        "# AUTOMATION OF FEATURE SELECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cq_xxKf5meN"
      },
      "outputs": [],
      "source": [
        "# class GenerateDatasetAutomation:\n",
        "#     def get_data(self, URL):\n",
        "\n",
        "#         # akses dataset melalui URL\n",
        "#         dataset = pd.read_csv(URL)\n",
        "\n",
        "#         # menentukan data dan kelas\n",
        "#         X = dataset.iloc[:, :-1]\n",
        "#         y = dataset.iloc[:, -1]\n",
        "\n",
        "#         # membagi dataset menjadi data training dan data testing\n",
        "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "#         # mengembalikan dataset yang sudah displit\n",
        "#         return X_train, X_test, y_train, y_test, URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF3RtSRL53SK"
      },
      "outputs": [],
      "source": [
        "# def GetAutomationResult():\n",
        "#     list_dataset = [\"spam.csv\", \"ionosphere.csv\", \"voice.csv\", \"parkinson.csv\", \"malware.csv\", \"mri.csv\", \"heart-attack.csv\", \"ozone-level.csv\"]\n",
        "\n",
        "#     for i in list_dataset:\n",
        "#         url = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/\" + i\n",
        "\n",
        "#         # get data training dan data testing dengan memanggil method\n",
        "#         X_train, X_test, y_train, y_test, URL = GenerateDatasetAutomation().get_data(url)\n",
        "\n",
        "#         # encoding\n",
        "#         X_train, y_train = Preprocessing().label_encoding(X_train, y_train)\n",
        "\n",
        "#         # cleaning data\n",
        "#         X_train, y_train = Preprocessing().get_clean_dataset(X_train, y_train)\n",
        "\n",
        "#         # implementasi normalisasi data pada data latih\n",
        "#         X_train, y_train, list_column_norm, scaler = Preprocessing().normalisasi(X_train, y_train)\n",
        "\n",
        "#         # 0: tidak memakai seleksi fitur, 1: memakai seleksi fitur\n",
        "#         for j in range(2):\n",
        "#             if j == 1:\n",
        "#                 # implementasi seleksi fitur\n",
        "#                 X_train, y_train, list_column_selected = Preprocessing().seleksi_fitur(X_train, y_train)\n",
        "#                 print(\"Jumlah fitur setelah SF: \", len(X_train.columns))\n",
        "#             else:\n",
        "#                 print(\"Jumlah fitur original: \", len(X_train.columns))\n",
        "\n",
        "#             # # membuang fitur yang tidak digunakan pada proses preprocessing\n",
        "#             # X_test = X_test[list_column_norm]\n",
        "\n",
        "#             # # encoding\n",
        "#             # X_test, y_test = Preprocessing().label_encoding(X_test, y_test)\n",
        "\n",
        "#             # # implementasi normalisasi data pada data uji\n",
        "#             # X_test, y_test = pd.DataFrame(scaler.transform(X_test), columns=list_column_norm), y_test.reset_index(drop=True)\n",
        "\n",
        "#             # # menggunakan kolom yang akan digunakan saja\n",
        "#             # if j == 1:\n",
        "#             #     X_test, y_test = X_test[list_column_selected], y_test\n",
        "\n",
        "#             # # generate model\n",
        "#             # start = time.time()\n",
        "#             # model_base, X_base, y_base = Train_Test_Process().train(X_train, y_train, classifier=\"ANN\")\n",
        "#             # end = time.time()\n",
        "\n",
        "#             # # get performa model pada data test\n",
        "#             # result_base = Train_Test_Process().test(model_base, X_base, y_base)\n",
        "\n",
        "#             # # print hasil\n",
        "#             # if j == 0:\n",
        "#             #     print(\"----------- DATASET \" + i + \" ORI -----------\")\n",
        "#             # else:\n",
        "#             #     print(\"----------- DATASET \" + i + \" DENGAN SF -----------\")\n",
        "\n",
        "#             # print(result_base)\n",
        "#             # print(\"Training time {:.3f} detik\" . format(end - start))\n",
        "#             # print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VslmQQ2_bbu"
      },
      "outputs": [],
      "source": [
        "# GetAutomationResult()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLWT3nrXb5X1"
      },
      "source": [
        "# AUTOMATION OF FILTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apq30aePcAW3"
      },
      "outputs": [],
      "source": [
        "class GenerateDatasetParallel:\n",
        "    def get_data(self, URL):\n",
        "\n",
        "        # akses dataset melalui URL\n",
        "        dataset = pd.read_csv(URL)\n",
        "\n",
        "        # menentukan data dan kelas\n",
        "        X = dataset.iloc[:, :-1]\n",
        "        y = dataset.iloc[:, -1]\n",
        "\n",
        "        # membagi dataset menjadi data training dan data testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
        "\n",
        "        # mengembalikan dataset yang sudah displit\n",
        "        return X_train, X_test, y_train, y_test, URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql0Fq6cIb-5J"
      },
      "outputs": [],
      "source": [
        "def GetParallelResult(n_neighbors, n_neighbors_filter, classifier):\n",
        "    list_dataset = [\"spam.csv\", \"ionosphere.csv\", \"voice.csv\", \"parkinson.csv\", \"malware.csv\", \"mri.csv\", \"heart-attack.csv\", \"ozone-level.csv\"]\n",
        "\n",
        "    for i in list_dataset:\n",
        "        url = \"https://raw.githubusercontent.com/adizaen/dataset-tesis/main/\" + i\n",
        "\n",
        "        # get data training dan data testing dengan memanggil method\n",
        "        X_train, X_test, y_train, y_test, URL = GenerateDatasetParallel().get_data(url)\n",
        "\n",
        "        # encoding\n",
        "        X_train, y_train = Preprocessing().label_encoding(X_train, y_train)\n",
        "\n",
        "        # cleaning data\n",
        "        X_train, y_train = Preprocessing().get_clean_dataset(X_train, y_train)\n",
        "\n",
        "        # implementasi normalisasi data pada data latih\n",
        "        X_train, y_train, list_column_norm, scaler = Preprocessing().normalisasi(X_train, y_train)\n",
        "\n",
        "        # implementasi seleksi fitur\n",
        "        X_train, y_train, list_column_selected = Preprocessing().seleksi_fitur(X_train, y_train)\n",
        "\n",
        "        # membuang fitur yang tidak digunakan pada proses preprocessing\n",
        "        X_test = X_test[list_column_norm]\n",
        "\n",
        "        # encoding\n",
        "        X_test, y_test = Preprocessing().label_encoding(X_test, y_test)\n",
        "\n",
        "        # implementasi normalisasi data pada data uji\n",
        "        X_test, y_test = pd.DataFrame(scaler.transform(X_test), columns=list_column_norm), y_test.reset_index(drop=True)\n",
        "\n",
        "        # menggunakan kolom yang akan digunakan saja\n",
        "        X_test, y_test = X_test[list_column_selected], y_test\n",
        "\n",
        "        # object dari class beberapa algoritma sampling\n",
        "        smote_enn = SMOTE_ENN(neighbors=n_neighbors)\n",
        "        wksmote_renn = WKSMOTE_RENN(neighbors=n_neighbors, n_neighbors_filter=n_neighbors_filter)\n",
        "\n",
        "        # proses training dengan menerapkan beberapa algoritma sampling di naive bayes\n",
        "        # model_base, X_base, y_base = Train_Test_Process().train(X_train, y_train, classifier=classifier)\n",
        "        model_smote_enn, X_smote_enn, y_smote_enn = Train_Test_Process().train(X_train, y_train, smote_enn, classifier=classifier)\n",
        "        model_wksmote_renn, X_wksmote_renn, y_wksmote_renn = Train_Test_Process().train(X_train, y_train, wksmote_renn, classifier=classifier)\n",
        "\n",
        "        # proses testing\n",
        "        # result_base = Train_Test_Process().test(model_base, X_test, y_test)\n",
        "        result_smote_enn = Train_Test_Process().test(model_smote_enn, X_test, y_test)\n",
        "        result_wksmote_renn = Train_Test_Process().test(model_wksmote_renn, X_test, y_test)\n",
        "\n",
        "        # proses plot\n",
        "        URL_DATA = URL.split(\"/\")[-1]\n",
        "\n",
        "        if URL_DATA == 'spam.csv':\n",
        "            column1 = \"word_freq_all\"\n",
        "            column2 = \"char_freq_$\"\n",
        "        elif URL_DATA == 'ionosphere.csv':\n",
        "            column1 = \"V8\"\n",
        "            column2 = \"V30\"\n",
        "        elif URL_DATA == 'colon-cancer.csv':\n",
        "            column1 = \"V53\"\n",
        "            column2 = \"V30\"\n",
        "        elif URL_DATA == 'voice.csv':\n",
        "            column1 = \"Jitter->pitch_PQ5_classical_Baken\"\n",
        "            column2 = \"HNR->HNR_dB_Praat_std\"\n",
        "        elif URL_DATA == 'parkinson.csv':\n",
        "            column1 = \"f1\"\n",
        "            column2 = \"tqwt_kurtosisValue_dec_21\"\n",
        "        elif URL_DATA == 'mri.csv':\n",
        "            column1 = \"obstime\"\n",
        "            column2 = \"plt\"\n",
        "        elif URL_DATA == 'heart-attack.csv':\n",
        "            column1 = \"BMI\"\n",
        "            column2 = \"PhysHlth\"\n",
        "        elif URL_DATA == 'ozone-level.csv':\n",
        "            column1 = \"V12\"\n",
        "            column2 = \"V59\"\n",
        "\n",
        "        print(\"Dataset: \", i)\n",
        "        visualize(result_smote_enn, result_wksmote_renn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeNFX_Hqc0Om"
      },
      "outputs": [],
      "source": [
        "GetParallelResult(n_neighbors=3, n_neighbors_filter = 5, classifier=\"NB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2ghjfJWOej3"
      },
      "outputs": [],
      "source": [
        "GetParallelResult(n_neighbors=3, n_neighbors_filter = 5, classifier=\"ANN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=4, n_neighbors_filter = 5, classifier=\"NB\")"
      ],
      "metadata": {
        "id": "gXhmGA1WA1K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=4, n_neighbors_filter = 5, classifier=\"ANN\")"
      ],
      "metadata": {
        "id": "m3prpal0A1ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=5, n_neighbors_filter = 5, classifier=\"NB\")"
      ],
      "metadata": {
        "id": "rqpteqNzA1hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=5, n_neighbors_filter = 5, classifier=\"ANN\")"
      ],
      "metadata": {
        "id": "tTAdOKiiA1lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=6, n_neighbors_filter = 5, classifier=\"NB\")"
      ],
      "metadata": {
        "id": "3l-wcXdvA1oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=6, n_neighbors_filter = 5, classifier=\"ANN\")"
      ],
      "metadata": {
        "id": "9Ufao-SNA5Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=7, n_neighbors_filter = 5, classifier=\"NB\")"
      ],
      "metadata": {
        "id": "-KaGWUQlA5QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetParallelResult(n_neighbors=7, n_neighbors_filter = 5, classifier=\"ANN\")"
      ],
      "metadata": {
        "id": "E4NPxHsVA1r8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "u2eNH8HBdXVT",
        "_JTuiw3HI2R8",
        "SJDMWuZ7I7bG",
        "t8YeTNTbI-na",
        "_gzeFsmuJTkZ",
        "PJsah8Q53Knp",
        "8I3PlaBs_Z8z",
        "HZqfKTU6Y-Da",
        "e_bq01X-5iyG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}